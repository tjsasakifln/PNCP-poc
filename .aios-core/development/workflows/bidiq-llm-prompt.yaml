workflow:
  id: bidiq-llm-prompt
  name: BidIQ LLM Prompt Engineering
  description: >-
    Agent workflow for creating, testing, and iterating on LLM prompts for BidIQ.
    Covers the full cycle: define objective, implement prompt + schema, test harness,
    evaluate outputs, refine, and validate with fallback behavior.
  type: brownfield
  project_types:
    - llm-integration
    - prompt-engineering
    - ai-feature

  sequence:
    - step: define_prompt_objective
      phase: 1
      phase_name: Objective Definition
      agent: analyst
      action: define prompt objective, constraints, and expected output format
      creates: docs/llm/prompt-objective.md
      notes: >-
        Definir claramente o objetivo do prompt: qual output esperado, restrições
        (max_tokens, formato), exemplos de entrada/saída, e critérios de sucesso.
        Considerar o contexto BidIQ: resumo executivo de licitações PNCP,
        schema ResumoLicitacoes (Pydantic), limite de 50 licitações por chamada.

    - step: implement_prompt_template
      phase: 2
      phase_name: Prompt Implementation
      agent: dev
      action: implement prompt template and Pydantic structured output schema
      creates: backend/llm.py updates, backend/schemas.py updates
      notes: >-
        Implementar system_prompt e user_prompt no llm.py. Usar Pydantic structured
        output (ResumoLicitacoes schema). Configurar parâmetros: model=gpt-4.1-nano,
        temperature=0.3, max_tokens=500. Truncar objetoCompra a 200 chars.

    - step: create_test_harness
      phase: 3
      phase_name: Test Harness
      agent: dev
      action: create test harness with sample inputs, expected outputs, and edge cases
      creates: backend/tests/test_llm_prompt.py
      notes: >-
        Criar test harness com: inputs de amostra (0 licitações, 1, 50, mix de UFs),
        outputs esperados, edge cases (texto muito longo, caracteres especiais,
        valores extremos). Incluir mock da OpenAI API para testes determinísticos.

    - step: evaluate_outputs
      phase: 4
      phase_name: Output Evaluation
      agent: qa
      action: evaluate LLM outputs for accuracy, consistency, hallucination, and token usage
      creates: docs/llm/prompt-evaluation.md
      notes: >-
        Avaliar: precisão dos números (total_oportunidades, valor_total),
        consistência entre execuções, detecção de alucinações, uso de tokens
        (deve ficar < 500), qualidade do resumo_executivo e destaques.
        Verificar que outputs seguem o schema Pydantic sem erros.

    - step: refine_prompt
      phase: 5
      phase_name: Prompt Refinement
      agent: dev
      action: refine prompt based on evaluation feedback
      creates: backend/llm.py updates
      notes: >-
        Ajustar com base na avaliação: temperatura, max_tokens, system_prompt,
        few-shot examples se necessário. Manter compatibilidade com schema
        ResumoLicitacoes. Documentar mudanças e justificativas.

    - step: final_validation
      phase: 6
      phase_name: Final Validation
      agent: qa
      action: run full test suite and verify fallback without LLM works
      creates: test results, coverage report
      notes: >-
        Executar pytest completo incluindo testes de prompt. Verificar que
        fallback sem LLM (quando OPENAI_API_KEY ausente) continua funcional.
        Confirmar coverage >= 70%. Validar comportamento com API real se possível.

  decision_guidance:
    when_to_use:
      - Creating new LLM prompts for BidIQ features
      - Iterating on existing prompt quality (resumo executivo)
      - Changing structured output schema (ResumoLicitacoes)
      - Optimizing token usage or reducing hallucinations
      - Adding new LLM-powered features (e.g., bid classification)

  handoff_prompts:
    analyst_to_dev: "Prompt objective defined with constraints and examples. Implement the prompt template and Pydantic schema."
    dev_to_qa_evaluate: "Prompt implemented with test harness. Evaluate outputs for accuracy, consistency, and hallucination."
    qa_to_dev_refine: "Evaluation complete with findings. Refine prompt based on feedback."
    dev_to_qa_validate: "Prompt refined. Run full validation including fallback behavior."
    complete: "LLM prompt validated and ready for production. All tests passing with coverage >= 70%."
