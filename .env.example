# =============================================================================
# BidIQ Uniformes POC - Environment Configuration Template
# =============================================================================
#
# SETUP INSTRUCTIONS:
# 1. Copy this file to `.env`: cp .env.example .env
# 2. Fill in the required values (see comments below)
# 3. NEVER commit `.env` to version control
# 4. Keep this template updated when adding new variables
#
# =============================================================================

# -----------------------------------------------------------------------------
# OpenAI API Configuration
# -----------------------------------------------------------------------------
# ⚠️ SECURITY: Never commit real API keys to git
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-key-here

# -----------------------------------------------------------------------------
# Backend Configuration
# -----------------------------------------------------------------------------
# Port for FastAPI backend server (default: 8000)
BACKEND_PORT=8000

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# PNCP Client Configuration
# -----------------------------------------------------------------------------
# Request timeout in seconds (default: 30)
# How long to wait for PNCP API response before giving up
PNCP_TIMEOUT=30

# Maximum number of retry attempts (default: 5)
# How many times to retry failed requests before failing permanently
PNCP_MAX_RETRIES=5

# Base delay for exponential backoff in seconds (default: 2)
# Initial wait time between retries (doubles each attempt)
PNCP_BACKOFF_BASE=2

# Maximum delay for exponential backoff in seconds (default: 60)
# Maximum wait time between retries (caps exponential growth)
PNCP_BACKOFF_MAX=60

# -----------------------------------------------------------------------------
# LLM Configuration (GPT-4.1-nano)
# -----------------------------------------------------------------------------
# OpenAI model to use for summary generation (default: gpt-4.1-nano)
LLM_MODEL=gpt-4.1-nano

# Temperature for LLM responses (0.0-1.0, default: 0.3)
# Lower = more deterministic, Higher = more creative
LLM_TEMPERATURE=0.3

# Maximum tokens for LLM response (default: 500)
# Controls length of generated summary
LLM_MAX_TOKENS=500

# -----------------------------------------------------------------------------
# Frontend Configuration (Next.js)
# -----------------------------------------------------------------------------
# Backend API URL for frontend to connect to (default: http://localhost:8000)
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000

# =============================================================================
# END OF CONFIGURATION
# =============================================================================
